{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from policy_iteration_probabilistic import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we try to find the optimal policy in a windy gridworld. We also study the effect of rewards on how the optimal policy behaves at the probabilisitc state i.e. cell (1, 2).\n",
    "\n",
    "The method to find the optimal policy is the same as in the [09_policy_iteration_deterministic.ipynb](./09_policy_iteration_deterministic.ipynb) notebook.\n",
    "\n",
    "Note: \n",
    " * the optimal policy we want to find is deterministic, only the environment is probabilistic\n",
    " * cell (0,0) is the top left corner of the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to be clear show the one state that has a probabilistic state transition\n",
    "g = windy_grid_penalized(0)\n",
    "\n",
    "for (current_state, action), v in g.probs.items():\n",
    "    if len(v) > 1:\n",
    "        for (future_state, prob) in v.items():\n",
    "            r = g.rewards.get(future_state, 0)\n",
    "            print(f\"s: {current_state}\")\n",
    "            print(f\"a: {action}\")\n",
    "            print(f\"s': {future_state}\")\n",
    "            print(f\"r: {r}\")\n",
    "            print(f\"p(s', r | s, a): {prob}\")\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, when you are at state (1, 2), taking action 'U' is risky because there is a 50% chance that you might end up at the terminal state with a -1 reward i.e. state (1, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(penalty=0):\n",
    "    grid = windy_grid_penalized(penalty)\n",
    "\n",
    "    print(\"the world is:\")\n",
    "    print_world(grid)\n",
    "    print(\"\")\n",
    "\n",
    "    policy = create_random_policy(grid)\n",
    "\n",
    "    solve_gridworld(grid, policy, gamma=0.9)\n",
    "    \n",
    "interact_manual(f, penalty=(-2, 0, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some interesting observations:\n",
    "    \n",
    "* when penalty is 0, we do not take any risk at (1, 2). We go down and around X to get the terminal state at (0, 3).\n",
    "* when penalty is -2 and you are at (1, 2), we go to the terminal state with -1 reward because any movement to (0, 3) will incur a higher expected cost.\n",
    "* when penalty is -0.2 and you are at (1, 2), we take a risk and go up!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
